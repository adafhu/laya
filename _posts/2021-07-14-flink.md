---
layout:     post
title:      "Flink"
subtitle:   "Flink Basic Info"
date:       2021-07-14 11:03:00
author:     "kgzhang"
catalog: false
category: bigData
header-style: text
tags:
  - flink
  - bigData
---

## Flink 基础

### 相关链接
- [Doc v1.11](https://ci.apache.org/projects/flink/flink-docs-release-1.11/try-flink/datastream_api.html)
- [Doc v1.14](https://nightlies.apache.org/flink/flink-docs-release-1.14//docs/try-flink/local_installation/)
- [Flink 下载链接](https://flink.apache.org/downloads.html)

### 部署环境
服务器已安装Java8 或 Java11, 参见[Try Flink](https://nightlies.apache.org/flink/flink-docs-release-1.14//docs/try-flink/local_installation/). 需配置好 `JAVA_HOME` 等环境变量, 参见 [java install](https://kougazhang.github.io/java/2021/05/31/java/)

**集成 Hadoop**

如果 Flink 要使用 Hadoop 的相关功能，比如 YARN 或 HDFS, Flink 的发行包并没有捆绑（bundle）Hadoop 的相关类，需要进行配置。

配置 Hadoop 的相关类，有 2 种方式：

**1. 通过环境变量 HADOOP_CLASSPATH**

如果 Flink 是与 HADOOP 混合部署，那么可以采取以下方式配置环境变量：

```shell
# 注意：hadoop 指的是 hadoop 二进制可执行文件，classpath 是传递给 hadoop 的参数
# hadoop classpath 会打印出 hadoop 需要的 class path
export HADOOP_CLASSPATH=`hadoop classpath`
```

**2. 使用 jar 包 flink-shaded-hadoop-2-uber**

> 注意：flink-shaded-hadoop-2-uber 包不在 Flink 主要的发行极坏内，所以不建议使用这种方式。

jar 包集成 Hadoop 的场景：Flink 与 Hadoop 独立部署。Flink 所在的机器上不想安装 Hadoop 相关依赖。

flink-shaded-hadoop-2-uber jar 包的下载链接：
- 如果使用的 Hadoop 是 2.x 版本，可以直接从 Flink 官方网站上下载，[链接](https://flink.apache.org/downloads.html#additional-component)
- 如果使用的 Hadoop 是 3.x 版本，可以从 Maven 仓库上下载，[链接](https://mvnrepository.com/artifact/org.apache.flink/flink-shaded-hadoop-3-uber)

已验证以下版本都支持 jar 包这种方式:
- 1.11
- 1.14

**3. 客户端**
本地开发环境需要在 pom.xml 中添加以下配置:

```shell
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>2.8.3</version>
    <scope>provided</scope>
</dependency>
```

参考：[Hadoop Integration](https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html)

Refer:
- [Hadoop Integration](https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/deployment/hadoop.html)

### Flink 各下载包的区别
- Apache Flink 1.14.0 for Scala 2.12, 编译后的包，该包内置 1 个 Scala 解释器. 部署 Flink 时需要选择这种包.
- Apache Flink 1.14.0 Source Release，Flink 源码，未编译。

### 编译包的目录结构
- ##bin/## 目录包含 flink binary 及管理 job 和 task 的脚本
 - flink, Flink 命令行工具，可以用来执行 flink job 的 JAR 包。(例子参见下面的 "提交 Flink Job")
 - start-cluster.sh, stop-cluster.sh 脚本内容待补充.

- ##conf## 目录包含配置文件
- ##examples/## 目录包含使用 Flink 的例子.

### 启动和停止本地集群

启动集群，Flink 将以后台进程运行

```shell 
./bin/start-cluster.sh
```

Flink Web 控制台地址: localhost:8081

停止集群

```shell 
./bin/stop-cluster.sh
```

### 提交 Flink job

提交 examples 中的单词统计的任务到刚才的启动的本地集群.

```shell 
./bin/flink run examples/streaming/WordCount.jar
```

提交成功后就可以在 Flink Web 控制台上看到刚才提交的任务。


## 读取文件

`readTextFile` 支持 gzip 等格式

## 生成项目骨架
[Fraud Detection with the DataStream API](https://ci.apache.org/projects/flink/flink-docs-release-1.11/try-flink/datastream_api.html)
```shell 
mvn archetype:generate \
    -DarchetypeGroupId=org.apache.flink \
    -DarchetypeArtifactId=flink-walkthrough-datastream-java \
    -DarchetypeVersion=1.11.2 \
    -DgroupId=<ReversedCompanyDomain> \
    -DartifactId=<projectName> \
    -Dversion=0.1 \
    -Dpackage=spendreport \
    -DinteractiveMode=false
```

## flink job 打包
```shell 
mvn clean package
```

## standalone 模式
在 standalone 模式下, 如果 taskmanager 节点重试若干次后连接不到 jobmanager 节点, taskmanager 节点会退出.

## Flink job 开发中遇到的问题

### 序列化失败
![/img/log-util.png](/img/log-util.png)

原因分析
 
`log.Util` 在实现 `FlatMapFunction` 时用到了, 它是一个自定义的工具类. 抛出异常的原因是 `log.Util` 没有实现 java 的 `Serializable` 接口, 所以 Flink 不能对该类进行序列化.

解决方法

`log.Util` 实现 `Serializable` 接口即可.

```java
import java.io.Serializable;

class Util implement Serializable {
    // ....
}
```

## Flink Stream

- Flink Hive: https://ci.apache.org/projects/flink/flink-docs-master/docs/connectors/table/hive/overview/

