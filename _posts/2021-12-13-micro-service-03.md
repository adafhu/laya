---
layout:     post
title:      "Microservice-03: gRPC & Service Discovery""
subtitle:   "gRPC & 服务发现"
date:       2021-12-13 17:04:35
author:     "kgzhang"
catalog: false
category: golang
header-style: text
tags:
  - golang
---

## gRPC

gRPC 的特点：
- 跨语言。安卓和 IOS 也可以使用。这样不必做内部服务到 Restful API 的转换。
- 协议支持 Protocol Buffer （PB）和 JSON。
- 可插拔。支持内部的插件，扩展负载均衡、服务发现。
- IDL: 基于文件定义服务，通过 proto3 工具生成对应语言的代码。用户端和服务端可以共享此 IDL 文件，不会像 HTTP API 接口和文档之间常常会脱节。IDL 文件可以定义接口的入参和返回值，还可以加注释。可以作为上下游沟通接口的文档约定。
- 设计理念：
    - 基于 HTTP2.0，能复用链接，更加高性能。http1.x 如果有 n 个 consumer，m 个 provider, 每个 consumer 维护大小为 50 的连接池，那么它们之间总共有 n * m * 50 个链接。HTTP2 支持单个 TCP 的多路复用。（标准库的 net/rpc 也实现了单个 TCP 的多路复用）。大致原理是：1 个链接上标识请求的 ID，比如 1 个 RPC 的请求的 call ID 100，在 RPC response 时也会带上 100，每个上层链接的 ID 不同，这样就可以知道这个 TCP 包是属于谁的。
    - 支持 Service Push。
- 传输层弱依赖，便于后期升级为 QUIC 等更高性能的 HTTP 协议。
- 服务而非对象，消息而非引用：IDL 中定义的是 service 和 message，和原来巨石架构中引用对象是不同的。服务之间使用粗粒度消息交互。
- 负载无关的：gRPC 支持多种协议，不只是 Protocol Buffer, 也可以是 JSON、XML，这样便于调试和测试。
- 流：Streaming API。
- 阻塞式和非阻塞式。支持同步和异步
- 元数据交换。元数据交互可以理解为 HTTP 的 Header，可以放一些业务无关的内容。比如鉴权、认证或 tracing。
- 标准化状态码：痛点是 HTTP 状态码和业务状态码混淆乱用。推荐是使用标准状态码来做大类区分。
- 性能问题。性能随着使用人多、社区大，慢慢就变好了。

## gRPC - HealthCheck

### 为什么 Health Check 非常重要？

RPC 里很重要的一个点是健康检测。

gRPC 有一个标准的健康检测协议，在 gRPC 的所有语言实现中基本提供了生成代码和用于设置运行状态的功能。也就是说，可以自己扩展它健康检测的协议。

为什么健康检测非常重要？
- 主动健康检测 health check, 可以在服务提供者不稳定时，被消费者感知，临时从负载均衡中摘除，减少错误请求。当服务提供者稳定后，health check 成功，重新加入到消费者的负载均衡中，恢复请求。(health check 通过剔除不稳定的服务提供者，使得服务整体更加稳定。)
- health check，同样也被用于外挂方式的容器健康检测，或者流量检测（k8s liveness & readiness）。
    - liveness: 指一个服务是活着还是挂了
    - readiness: 服务是否 ready 可以接收流量

### Health Check 在服务发现中的作用

Health Check 在服务发现领域也非常有用！

为什么需要服务发现？

在没有服务发现前，在微服务上线后把 3 个IP（三线）写入 Nginx 或使用 DNS 去访问我们的服务提供者，在服务比较少时也是可以接受的。(感觉这里讲错了，因为 3 个 IP 指三线的话是暴露在公网了，使用 DNS 去访问我们的服务提供者还差不多)

随着流量增大，节点增多，对于动态扩缩容的需求也越来越强，固定 IP 的方式越来越不方便。为了解决动态扩缩容使用了 K8S，K8S 上每次发布应用 IP 都会变化，所以就需要服务发现动态地获取当前应用的地址。

服务发现的构成
- Provider: 把服务名、服务协议（HTTP/RPC）、IP 和 PORT, 元数据（机房和集群信息、权重）等注册到服务发现 Discovery.
- Consumer: 从 Discovery 中获取对应服务的信息。

为什么在服务发现中需要 Health Check?

假设 Provider 和 Consumer 之间的网络出现问题，但是它们与服务发现之间是相通的，这时如果没有 Health Check 机制，Provider 还是向 Consumer 发送请求的话就会报错。

### Health Check 在应用平滑发布中的作用

Lame Duck: 在政治中，lame duck 是指一个官员他的继任者已经或即将被选出。在这里指一个应用即将下线或即将上线，是一种特殊的状态不能正常地对外提供服务。

滚动发布：逐步更新应用直到所有的应用都更新完毕。

Health Check 在平滑发布中的作用？
- K8S 向应用发一个 SignalQuit 的信号；
- 应用拦截到这个信号后，向服务发现发送一个注销服务的请求
- 应用有一个全局变量把自己标识为 “lame duck”, 这个全局变量也可以被应用的 Health Check 接口所访问。这时有 consumer 再访问这个应用之前调用了这个应用的 Health Check 接口，就会发现这个服务不可以。(应用已经向服务发现申请注销了，为什么还要标识“lame duck” 状态，因为可能 consumer 向 discovery 请求时，discovery 尚未收到 pvd 的注销请求)
- InFlight: 应用正在处理的请求。应用要等没有 active 的链接时再下线。
- 兜底策略：K8S 超过一定时间发现容器还是没有下线就会强杀该服务。

服务上线：
- 服务待上线进入跛脚鸭状态，经过 JVM 预热等，完全准备好了，Health Check 就成功了。

## 服务发现的几种模型

### 客户端发现
- 服务端和客户端是直连的，没有使用集中式的负载均衡转发请求。
- 服务端实例启动时会注册到服务发现上；
- 客户端从服务发现上捞取可用的服务端地址，建立一个连接池，使用本地的负载均衡策略使用服务端地址；
- 服务端实例下线时会向服务发现申请注销。客户端有定时机制同步服务发现的最新数据，这样就可以感知到哪些服务端实例的变化了

### 服务端发现
相关选型有：Consul Template+Nginx, K8S+etcd

服务端发现就和普通的反向代理没啥区别了，都是客户端去把请求发送到服务端代理，服务端代理再按照负载均衡策略进行分发。

弊端：
- 这种集中式的服务发现违背了之前讲到的去中心化的治理。去中心的治理是指去数据中心或去流量中心，不让某一个组件成为热点。
- 集中式负载均衡的策略对于公网来说还可以接受，但是对于内网来说不合适。因为内网的流量要比公网大很多。1 个业务请求请来可能会造成内网十倍量的请求。

优势：上游调用简单，代码无入侵。

### Service Mesh: 服务网格

Service Mesh 是为了解决中心化的负载均衡代理的问题。

服务代理是把代理 proxy 和服务部署到同一台物理机或 K8S 的 pod 上，服务把消息先发给 proxy，proxy 再直连到服务的下游的消费者。

这样把之前需要写大量的服务发现和负载均衡的代码搬到了 proxy 中，而且又是去中心化部署的。集中式的服务发现被称为 Smart Client, 因为要做熔断、限流等工作。

### 服务发现模型选型
微服务的核心是去中心化。建议使用客户端发现模型。

Service Mesh 只有体量更大时才可能带来收益。

### 服务发现总结
为什么 Zookeeper 不适合用来做服务发现？
- Zookeeper 是 CP 系统, 为了维护 CP，在某些情况下它是不能对外提供服务的，这就导致:
    - 新的节点无法注册，老节点无法及时下掉。 
    - 如果 Consumer 没有把 Pvd 的信息缓存到本地而是每次都会向 Zookeeper 获取请求地址，这就会导致 Consumer 也不可用（实际上不会发生的，基本上 RPC 框架都会实现本地缓存的功能, 这被称为 Client snapshot）
    - Zookeeper 一旦发生网络分区，重选 master 的时间会非常长的，可能会有几十秒
    - Zookeeper 一旦跨机房部署，就会导致它的心跳机制不可靠。因为心跳包的延迟会增大，这就会造成网络抖动。如果把心跳的间隔调大，那就好比人的反应变慢，神经变得迟钝，对于有老的节点下线也不能及时感知到。
    - 使用 Zookeeper 做服务发现是通过 “临时节点+watch” 的机制，如果有 1k consumer、1k pvd，1k Zookeeper，Zookeeper 广播的量非常大，容易发生 full GC，此时也要重新选主。

为什么服务发现适合使用 AP？
- 如果发生 AP，新上线的节点不能及时被发现，只会造成负载不均衡。
- 如果发生 AP，待下线的节点不能及时下线，但是由于前面介绍过的 Health Check 双兜底的机制，并且待下线的节点进入 lame duck 状态后。。。造成的影响也小的多。

B 站自己实现的服务发现的思路（参照了 Eureka）
- 通过 Family(AppID) 和 Addr (IP:Port) 定位实例，初次之外还可以附带更多的元数据：权限、染色、集群。
    - AppID：使用三段式命名, business.service.xxx。按照业务命令。如：Gmail.account.serice
- Service Provider 有三种事件，Register(注册服务)、Renew（刷新心跳）、Cancel（注销服务）
- 服务发现故障时，Provider 不建议重启和发布。这样的话会导致 Consumer 连接不上，引起事故。（服务发现暂时性故障没关系，因为 Consumer 会有本地的 Provider 的 Snapshot）
- Consumer 启动时拉取实例，发起 30s 长轮询。

Eruka 的缺点：
- 1.0 时会同步所有的信息。B 站有10万节点，下载的量有点大了。
- 解决方案：
    - 读写分离
    - 数据 sharding，划出一个地方来专门写数据
- 长轮询不如 push 更节省资源.


### Web 应用获取实时更新的几种方式
- 短轮询：按照指定时间间隔不断地发送请求，比如每秒 1 次；
- 长轮询：与短轮询相比，每个请求的存活时间比较长。只有服务端更新或请求超时，该请求才会被响应。（也就是发了请求之后 service hang 住它）
- HTTP 流：Web 应用发起一个持久的 HTTP 请求，且请求不会被关闭。
- WebSocket: Web 应用发起一个持久的允许双向通信的请求。
- 剔除垃圾节点的机制：在一段时间内，Pvd 都没有来刷新，就认为Pvd 失效了，把它踢下线。
- 在应用更新时，不要把所有节点一下子全部踢下线，应该逐步摘除节点。
- 自保护机制，当节点大面积失联时，服务发现不会把所有节点都踢下线，过期的数据比没有的数据好得多。


### CAP 理论
CAP理论中的P到底是个什么意思？ - 四猿外的回答 - 知乎
https://www.zhihu.com/question/54105974/answer/1643846752

CAP: Consistent 一致的，Available 可用的，Partition 分区

Consistent 一致性
- 一致性是指数据能一起变化，能让数据整齐划一。
- 数据什么什么时候才会变化？数据只有在写请求的时候才会变化
- 数据一致性的判断标准是什么？在系统正常情况下，写请求修改了多个节点上的数据时，读请求都能读取到这些节点上的变化；当系统不正常时系统数据不一致时，系统为了维护数据的一致性，对读请求不返回任何数据。

Available 可用性
- 可用性不区分是写请求和读请求，它要求系统内的节点能响应，有2点约束：
    - 返回结果必须在合理的时间以内，这个合理时间是由业务决定的
    - 系统内能正常接收请求的所有节点都返回结果，有2重含义：
        - 部分节点宕机不影响可用性
        - 节点能正常接收请求，但是发现节点内数据有问题，也必须返回结果。

Patition 分区容忍度
- 分布式系统有很多节点，节点之间通过网络进行通信。当节点间的网络通信出现问题时，称当前的分布式系统出现了分区。


CAP 对分布式系统的指导
- 分区是必然存在的，因为网络通信必然是不稳定的。所以分布式系统设计时是在 AP 和 CP 之间进行选择。
- zookeeper、分布式数据库都是 CP 系统
- Eureka、互联网内场景的微服务经常是 AP 系统

常见误解
- 分布式系统因为 CAP 定理放弃了 C 或 A 中的一个。不是的，因为P 也是只有故障时才发现的，当没有 P 时更不会有 A 和 C，系统正常状态下是完美的。
- C 和 A 之间的选择是针对整个分布式系统的，只能整体考虑 C 和 A。不是的，当分区发生时，其实是对一致性和可用性的选择是局部的，不是针对整个系统的。比如记账系统肯定是 CP，比如是用户头像信息等系统分区时选择 AP。
- CAP是 C 和 A 之间的二元对立选择。不是的，C 和 A 都只是一个范围，比如系统认定 P 发生的条件不是某个节点某次请求出现问题就认为出现了分区，而是要经过大多数投票的。

为什么平时区分 CAP 系统如此之难？

因为 AP 或 CP 的选择，只局限为分布式系统的局部或某些特殊的数据，在实际的工程实践中，分布式系统整体上会既保障 A 又保证 C 的。

在实践中以及后来 CAP 定理的提出者也承认，一致性和可用性并不仅仅是二选一的问题，只是一些重要性的区别，当强调一致性的时候，并不表示可用性是完全不可用的状态。比如，Zookeeper 只是在 master 出现问题的时候，才可能出现几十秒的不可用状态，而别的时候，都会以各种方式保证系统的可用性。而强调可用性的时候，也往往会采用一些技术手段，去保证数据最终是一致的。

